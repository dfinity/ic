//! Quic Transport.
//!
//! Transport layer based on QUIC. Provides connectivity to all peers in a subnet and
//! the ability to do rpc's to any peer in the subnet. RPC's are routed to the corresponding
//! handlers. Each RPC occurs on a different substream and are therefore fully decoupled from
//! each other.
//!
//! COMPONENTS:
//!  - Connection Manager (connection_manager.rs): Keeps peers connected.
//!  - Request Handler (request_handler.rs): Accepts streams on an active connection.
//!    Spawned by the connection manager for each connection.
//!  - Connection Handle (connection_handle.rs): Provides rpc and push interfaces to a peer.
//!
//! API:
//!  - Constructor takes a topology watcher. The topology defines the
//!    set of peers, to which transport tries to keep active connections.
//!  - Constructor also takes a Router. Incoming requests are routed to a handler
//!    based on the URI specified in the request.
//!  - `get_conn_handle`: Can be used to get a `ConnectionHandle` to a peer.
//!     The connection handle is small wrapper around the actual quic connection
//!     with an rpc/push interface. Passed in requests need to specify an URI to get
//!     routed to the correct handler.
//!
//! GUARANTEES:
//!  - If a peer is reachable, part of the topology and well-behaving transport will eventually
//!    open a connection.
//!  - The connection handle returned by `get_conn_handle` can be broken.
//!    It is responsibility of the transport user to have an adequate retry logic.
//!
//!
use std::{
    collections::{BTreeSet, HashMap},
    fmt::Debug,
    future::Future,
    net::SocketAddr,
    sync::{Arc, RwLock},
};

use async_trait::async_trait;
use axum::{
    http::{Request, Response},
    Router,
};
use bytes::Bytes;
use ic_base_types::{NodeId, RegistryVersion};
use ic_crypto_tls_interfaces::TlsConfig;
use ic_interfaces_registry::RegistryClient;
use ic_logger::{info, warn, ReplicaLogger};
use ic_metrics::MetricsRegistry;
use phantom_newtype::AmountOf;
use quinn::{
    AsyncUdpSocket, ConnectionError, ReadError, ReadToEndError, SendStream, StoppedError, VarInt,
    WriteError,
};
use thiserror::Error;
use tokio::sync::watch;
use tokio::task::{JoinError, JoinHandle};
use tokio_util::{sync::CancellationToken, task::task_tracker::TaskTracker};
use tracing::instrument;

use crate::connection_handle::ConnectionHandle;
use crate::connection_manager::start_connection_manager;
use crate::metrics::QuicTransportMetrics;

mod connection_handle;
mod connection_manager;
mod metrics;
mod request_handler;
pub use crate::connection_manager::create_udp_socket;

/// On purpose the value is big, otherwise there is risk of not processing important consensus messages.
/// E.g. summary blocks generated by the consensus protocol for 40 node subnet can be bigger than 5MB.
pub(crate) const MAX_MESSAGE_SIZE_BYTES: usize = 128 * 1024 * 1024;

/// The shutdown primitive is useful if futures should be cancelled at places different than '.await' points.
/// Such functionality is needed to have explicit control on the state when exiting.
pub struct Shutdown {
    cancellation: CancellationToken,
    task_tracker: TaskTracker,
    join_handle: JoinHandle<()>,
}

impl Shutdown {
    /// If a panic happens it should be propagated upstream.
    /// https://github.com/tokio-rs/tokio/issues/4516
    pub async fn shutdown(self) -> Result<(), JoinError> {
        // If an error is returned it means the conn manager is already stopped.
        self.cancellation.cancel();
        self.task_tracker.wait().await;
        self.join_handle.await
    }

    pub fn cancel(&self) {
        self.cancellation.cancel()
    }

    pub fn completed(&self) -> bool {
        self.task_tracker.is_closed() && self.task_tracker.is_empty()
    }

    pub fn spawn_on_with_cancellation<F>(
        run: impl FnOnce(CancellationToken) -> F,
        rt_handle: &tokio::runtime::Handle,
    ) -> Self
    where
        F: Future<Output = ()> + Send + 'static,
    {
        let task_tracker = TaskTracker::new();
        let cancellation = CancellationToken::new();
        let join_handle = task_tracker.spawn_on(run(cancellation.clone()), rt_handle);
        let _ = task_tracker.close();
        Self {
            cancellation,
            task_tracker,
            join_handle,
        }
    }
}

#[derive(Clone)]
pub struct QuicTransport {
    log: ReplicaLogger,
    metrics: QuicTransportMetrics,
    conn_handles: Arc<RwLock<HashMap<NodeId, ConnectionHandle>>>,
    shutdown: Arc<RwLock<Option<Shutdown>>>,
}

/// This is the main transport handle used for communication between peers.
/// The handler can safely be shared across threads and tasks.
///
/// Instead of the common `connect` and `disconnect`` methods the implementation
/// listens for changes of the topology using a watcher.
/// (The watcher matches better the semantics of peer discovery in the IC).
///
/// This enables complete separation between peer discovery and the core P2P
/// protocols that use `QuicTransport`.
/// For example, "P2P for consensus" implements a generic replication protocol which is
/// agnostic to the subnet membership logic required by the consensus algorithm.
/// This makes "P2P for consensus" a generic implementation that potentially can be used
/// not only by the consensus protocol of the IC.
impl QuicTransport {
    /// This is the entry point for creating (e.g. binding) and starting the quic transport.
    pub fn start(
        log: &ReplicaLogger,
        metrics_registry: &MetricsRegistry,
        rt: &tokio::runtime::Handle,
        tls_config: Arc<dyn TlsConfig + Send + Sync>,
        registry_client: Arc<dyn RegistryClient>,
        node_id: NodeId,
        // The receiver is passed here mainly to be consistent with other managers that also
        // require receivers on construction.
        topology_watcher: watch::Receiver<SubnetTopology>,
        udp_socket: Arc<dyn AsyncUdpSocket>,
        // Make sure this is respected https://docs.rs/axum/latest/axum/struct.Router.html#a-note-about-performance
        router: Router,
    ) -> QuicTransport {
        info!(log, "Starting Quic transport.");

        let conn_handles = Arc::new(RwLock::new(HashMap::new()));
        let metrics = QuicTransportMetrics::new(metrics_registry);

        let shutdown = start_connection_manager(
            log,
            metrics.clone(),
            rt,
            tls_config.clone(),
            registry_client,
            node_id,
            conn_handles.clone(),
            topology_watcher,
            udp_socket,
            router,
        );

        QuicTransport {
            log: log.clone(),
            metrics,
            conn_handles,
            shutdown: Arc::new(RwLock::new(Some(shutdown))),
        }
    }

    /// Graceful shutdown of transport
    pub async fn shutdown(&mut self) {
        let maybe_shutdown = self.shutdown.write().unwrap().take();
        if let Some(shutdown) = maybe_shutdown {
            let _ = shutdown.shutdown().await;
        }
    }
}

/// QUIC error code for stream cancellation. See
/// https://datatracker.ietf.org/doc/html/draft-ietf-quic-transport-03#section-12.3.
const QUIC_STREAM_CANCELLED: VarInt = VarInt::from_u32(0x80000006);

/// A drop guard that sends a [`SendStream::reset`] frame when dropped.
///
/// By default, QUINN sends a [`SendStream::finish`] frame upon dropping a [`SendStream`].
/// This struct overrides that behavior by sending a reset frame instead, signaling
/// that the message transmission was canceled. This approach helps optimize bandwidth
/// usage in scenarios involving message cancellation.
struct ResetStreamOnDrop {
    send_stream: SendStream,
}

impl ResetStreamOnDrop {
    fn new(send_stream: SendStream) -> Self {
        Self { send_stream }
    }
}

impl Drop for ResetStreamOnDrop {
    fn drop(&mut self) {
        // fails silently if the stream is already closed.
        let _ = self.send_stream.reset(QUIC_STREAM_CANCELLED);
    }
}

#[derive(Error, Debug)]
pub enum TransportError {
    // Occurs when the peer cancels the `RecvStream` or drops the `SendStream` future e.g., when the RPC method is part of a `select` branch.
    #[error("{0}")]
    StreamCancelled(String),
    /// This can occur during a topology change or when the connection manager attempts to replace an old, broken connection with a new one.
    #[error("Connection was closed")]
    ConnectionClosed(String),
    /// This can occur if the peer crashes or experiences connectivity issues.
    #[error("Connection timed out.")]
    TimedOut,
    #[error("Peer")]
    PeerNotFound,
    // If any of the following errors occur it means that we have a bug in the protocol implementation or
    // there is malicious peer on the other side.
    #[error("{0}")]
    Internal(String),
}

impl From<ConnectionError> for TransportError {
    fn from(err: ConnectionError) -> TransportError {
        match err {
            ConnectionError::LocallyClosed | ConnectionError::ApplicationClosed(_) => {
                TransportError::ConnectionClosed(format!("{:?}", err))
            }
            ConnectionError::TimedOut => TransportError::TimedOut,
            _ => TransportError::Internal(format!("{:?}", err)),
        }
    }
}

impl From<WriteError> for TransportError {
    fn from(err: WriteError) -> TransportError {
        match err {
            WriteError::Stopped(_) => TransportError::StreamCancelled(format!("{:?}", err)),
            WriteError::ConnectionLost(conn_err) => conn_err.into(),
            _ => TransportError::Internal(format!("{:?}", err)),
        }
    }
}

impl From<ReadError> for TransportError {
    fn from(err: ReadError) -> TransportError {
        match err {
            ReadError::Reset(_) => TransportError::StreamCancelled(format!("{:?}", err)),
            ReadError::ConnectionLost(conn_err) => conn_err.into(),
            _ => TransportError::Internal(format!("{:?}", err)),
        }
    }
}

impl From<StoppedError> for TransportError {
    fn from(err: StoppedError) -> TransportError {
        match err {
            StoppedError::ConnectionLost(conn_err) => conn_err.into(),
            StoppedError::ZeroRttRejected => TransportError::Internal(format!("{:?}", err)),
        }
    }
}

impl From<ReadToEndError> for TransportError {
    fn from(err: ReadToEndError) -> TransportError {
        match err {
            ReadToEndError::Read(read_err) => read_err.into(),
            ReadToEndError::TooLong => TransportError::Internal(format!("{:?}", err)),
        }
    }
}

#[async_trait]
impl Transport for QuicTransport {
    #[instrument(skip(self, request))]
    async fn rpc(
        &self,
        peer_id: &NodeId,
        request: Request<Bytes>,
    ) -> Result<Response<Bytes>, TransportError> {
        let _timer = self
            .metrics
            .connection_handle_duration_seconds
            .with_label_values(&[request.uri().path()])
            .start_timer();

        let bytes_sent_counter = self
            .metrics
            .connection_handle_bytes_sent_total
            .with_label_values(&[request.uri().path()]);
        bytes_sent_counter.inc_by(request.body().len() as u64);

        let bytes_received_counter = self
            .metrics
            .connection_handle_bytes_received_total
            .with_label_values(&[request.uri().path()]);

        let peer = self
            .conn_handles
            .read()
            .unwrap()
            .get(peer_id)
            .ok_or(TransportError::PeerNotFound)?
            .clone();
        let response_result = peer.rpc(request).await;
        match response_result {
            Ok(response) => {
                bytes_received_counter.inc_by(response.body().len() as u64);
                Ok(response)
            }
            Err(err) => {
                let counter = &self.metrics.connection_handle_errors_total;
                match &err {
                    TransportError::StreamCancelled(_) => {
                        counter.with_label_values(&["stream_cancelled"]).inc()
                    }
                    TransportError::ConnectionClosed(_) => {
                        counter.with_label_values(&["connection_closed"]).inc()
                    }
                    TransportError::TimedOut => {
                        counter.with_label_values(&["connection_timed_out"]).inc()
                    }
                    TransportError::PeerNotFound => {
                        counter.with_label_values(&["peer_not_found"]).inc()
                    }
                    TransportError::Internal(internal_err) => {
                        warn!(self.log, "{:?}", internal_err);
                        counter.with_label_values(&["internal"]).inc();
                    }
                }
                Err(err)
            }
        }
    }

    fn peers(&self) -> Vec<(NodeId, ConnId)> {
        self.conn_handles
            .read()
            .unwrap()
            .iter()
            .map(|(n, c)| (*n, c.conn_id()))
            .collect()
    }
}

/// Low-level transport interface for exchanging messages between nodes.
///
/// It intentionally uses http::Request and http::Response types.
/// By using them, HTTP servers build on top of Axum + TCP can be an easily transitioned to the quic transport.
#[async_trait]
pub trait Transport: Send + Sync {
    async fn rpc(
        &self,
        peer_id: &NodeId,
        request: Request<Bytes>,
    ) -> Result<Response<Bytes>, TransportError>;

    fn peers(&self) -> Vec<(NodeId, ConnId)>;
}

pub struct ConnIdTag {}
pub type ConnId = AmountOf<ConnIdTag, u64>;

#[derive(Copy, Clone, Default)]
pub enum MessagePriority {
    High,
    #[default]
    Low,
}

impl From<MessagePriority> for i32 {
    fn from(mp: MessagePriority) -> i32 {
        match mp {
            MessagePriority::High => 1,
            MessagePriority::Low => 0,
        }
    }
}

/// Holds socket addresses of all peers in a subnet.
#[derive(Clone, Eq, PartialEq, Debug, Default)]
pub struct SubnetTopology {
    subnet_nodes: HashMap<NodeId, SocketAddr>,
    earliest_registry_version: RegistryVersion,
    latest_registry_version: RegistryVersion,
}

impl SubnetTopology {
    pub fn new<T: IntoIterator<Item = (NodeId, SocketAddr)>>(
        subnet_nodes: T,
        earliest_registry_version: RegistryVersion,
        latest_registry_version: RegistryVersion,
    ) -> Self {
        Self {
            subnet_nodes: HashMap::from_iter(subnet_nodes),
            earliest_registry_version,
            latest_registry_version,
        }
    }

    pub fn iter(&self) -> impl Iterator<Item = (&NodeId, &SocketAddr)> {
        self.subnet_nodes.iter()
    }

    pub fn is_member(&self, node: &NodeId) -> bool {
        self.subnet_nodes.contains_key(node)
    }

    pub fn get_addr(&self, node: &NodeId) -> Option<SocketAddr> {
        self.subnet_nodes.get(node).copied()
    }

    pub fn latest_registry_version(&self) -> RegistryVersion {
        self.latest_registry_version
    }

    pub fn earliest_registry_version(&self) -> RegistryVersion {
        self.earliest_registry_version
    }

    pub fn get_subnet_nodes(&self) -> BTreeSet<NodeId> {
        self.subnet_nodes.keys().copied().collect()
    }
}
