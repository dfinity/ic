[sources.vector_metrics]
type = "internal_metrics"

[sinks.vector_exporter]
type = "prometheus_exporter"
inputs = ["vector_metrics"]
address = "${VECTOR_PROMETHUS_ADDR}"
default_namespace = "vector"
suppress_timestamp = true

# journal

[sources.journal]
type = "journald"
include_units = [
  "certificate-issuer",
  "certificate-syncer",
  "ic-boundary",
  "nginx",
  "danted",
]

# nginx

[transforms.nginx]
type = "filter"
inputs = ["journal"]
condition = "._SYSTEMD_UNIT == \"nginx.service\""

# nginx access

[transforms.nginx_access]
type = "filter"
inputs = ["nginx"]
condition = ".SYSLOG_IDENTIFIER == \"access\""

[transforms.nginx_access_preprocessed]
type = "remap"
inputs = ["nginx_access"]
source = """
. = parse_json!(.message)

# Inject the environment
.env = "${ENV:?ENV must be provided}"

# Anonymize remote_addr
remote_addr, err = to_string(.remote_addr)
if err != null || remote_addr == "" {
  .remote_addr_family = 0;
  .remote_addr_hashed = ""
} else {
  # Make sure it's properly parsed as either IPv4 or IPv6, otherwise fall back to 0
  .remote_addr_family = if is_ipv4(remote_addr) { 4 } else if is_ipv6(remote_addr) { 6 } else { 0 };

  .remote_addr_hashed = sha3("${IP_HASH_SALT:?IP_HASH_SALT must be provided}" + remote_addr, variant: "SHA3-224")
  .remote_addr_hashed = truncate(.remote_addr_hashed, limit: 32)
}

# Remove privacy related info
del(.remote_addr)

# parse status for later sampling
status, err = to_int(.status)
if err == null {
  .status = status
}

.ic_http_request =
  if .ic_method_name == "http_request" { 1 } else { 0 }
"""

[transforms.nginx_access_by_status]
type = "route"
inputs = ["nginx_access_preprocessed"]

[transforms.nginx_access_by_status.route]
1xx = '.status >= 100 && .status < 200 ?? false'
2xx = '.status >= 200 && .status < 300 ?? false'
3xx = '.status >= 300 && .status < 400 ?? false'
4xx = '.status >= 400 && .status < 500 ?? false'
5xx = '.status >= 500 && .status < 600 ?? false'

# nginx access (metrics)

[transforms.metrics_nginx_access_preprocessed]
type = "remap"
inputs = ["nginx_access_preprocessed"]
source = """
for_each([
  "ic_node_id",
  "ic_request_type",
  "ic_subnet_id",
]) -> |_, k| {
  if get!(., [k]) == "" {
    . = set!(., [k], "N/A")
  }
}
"""

[transforms.nginx_access_metrics]
type = "log_to_metric"
inputs = ["metrics_nginx_access_preprocessed"]

[[transforms.nginx_access_metrics.metrics]]
type = "counter"
field = "status"
name = "request_total"

[transforms.nginx_access_metrics.metrics.tags]
hostname = "{{ hostname }}"
ic_http_request = "{{ ic_http_request }}"
ic_node_id = "{{ ic_node_id }}"
ic_request_type = "{{ ic_request_type }}"
ic_subnet_id = "{{ ic_subnet_id }}"
is_bot = "{{ is_bot }}"
request_method = "{{ request_method }}"
status = "{{ status }}"

# nginx access (clickhouse)

[transforms.clickhouse_nginx_access_2xx_sampled]
rate = ${CLICKHOUSE_2XX_SAMPLE_RATE:?CLICKHOUSE_2XX_SAMPLE_RATE must be provided}

type = "sample"
inputs = ["nginx_access_by_status.2xx"]
[transforms.clickhouse_nginx_access_preprocessed]
type = "remap"
inputs = [
  "nginx_access_by_status.1xx",
  "clickhouse_nginx_access_2xx_sampled",
  "nginx_access_by_status.3xx",
  "nginx_access_by_status.4xx",
  "nginx_access_by_status.5xx",
  "nginx_access_by_status._unmatched",
]
source = """
# normalize https
.https = if .https == "on" { 1 } else { 0 }

# convert from float-second to int-ms
for_each([
  "connection_time",
  "request_time",
]) -> |_, k| {
  t_ms, err = to_float(get!(., [k])) * 1000
  if err != null {
    t_ms = null
  }

  . = set!(., [k + "_ms"], to_int(t_ms))
}

# rename field
.date = to_int(to_float!(.msec))
"""

[sinks.clickhouse_nginx_access]
type = "clickhouse"
inputs = ["clickhouse_nginx_access_preprocessed"]

# General
endpoint = "${CLICKHOUSE_URL:?CLICKHOUSE_URL must be provided}"
database = "default"
table = "http_access"

[sinks.clickhouse_nginx_access.healthcheck]
enabled = false

[sinks.clickhouse_nginx_access.batch]
max_bytes = 10485760 # 10 MB
max_events = 25000   # 25k
timeout_secs = 10

[sinks.clickhouse_nginx_access.buffer]
max_events = 100000 # 100k
type = "memory"
when_full = "block"

[sinks.clickhouse_nginx_access.request]
retry_attempts = 5
retry_initial_backoff_secs = 2

[sinks.clickhouse_nginx_access.auth]
strategy = "basic"
user = "${CLICKHOUSE_USER:?CLICKHOUSE_USER must be provided}"
password = "${CLICKHOUSE_PASSWORD:?CLICKHOUSE_PASSWORD must be provided}"

[sinks.clickhouse_nginx_access.encoding]
only_fields = [
  "body_bytes_sent",
  "bytes_sent",
  "content_length",
  "content_type",
  "date",
  "env",
  "geo_city_name",
  "geo_country_code",
  "geo_country_name",
  "hostname",
  "http_host",
  "http_origin",
  "http_referer",
  "http_user_agent",
  "https",
  "ic_canister_id",
  "ic_canister_id_cbor",
  "ic_http_request",
  "ic_method_name",
  "ic_node_id",
  "ic_request_type",
  "ic_sender",
  "ic_subnet_id",
  "is_bot",
  "pre_isolation_canister",
  "query_string",
  "remote_addr_hashed",
  "remote_addr_family",
  "request_id",
  "request_length",
  "request_method",
  "request_time_ms",
  "request_uri",
  "server_protocol",
  "ssl_cipher",
  "ssl_protocol",
  "status",
]

# nginx error

[transforms.nginx_error]
type = "filter"
inputs = ["nginx"]
condition = ".SYSLOG_IDENTIFIER == \"error\""

[transforms.nginx_error_clean]
type = "filter"
inputs = ["nginx_error"]
condition = "!contains(string!(.message), \"closed keepalive connection\")"

[transforms.nginx_error_json]
type = "remap"
inputs = ["nginx_error_clean"]
source = """
.@timestamp, err = to_int(.__REALTIME_TIMESTAMP)
if err != null {
  .@timestamp = null
}

.@timestamp, err = .@timestamp / 1000
if err != null {
  .timestamp = null
}
.@timestamp = to_int(.@timestamp)

. = {
  "@timestamp": .@timestamp,
  "host": .host,
  "message": .message
}
"""

# nginx error (metrics)

[transforms.nginx_error_metrics]
type = "log_to_metric"
inputs = ["nginx_error_json"]

[[transforms.nginx_error_metrics.metrics]]
type = "counter"
field = "message"
name = "error_total"

[transforms.nginx_error_metrics.metrics.tags]
hostname = "{{ host }}"

# nginx (prometheus)

[sinks.prometheus_exporter_nginx]
type = "prometheus_exporter"
inputs = ["nginx_access_metrics", "nginx_error_metrics"]
address = "${NGINX_PROMETHUS_ADDR?:NGINX_PROMETHUS_ADDR must be provided}"
default_namespace = "nginx"
suppress_timestamp = true

# danted (socks proxy)

[transforms.danted]
type = "filter"
inputs = ["journal"]
condition = "._SYSTEMD_UNIT == \"danted.service\""

[transforms.danted_json]
type = "remap"
inputs = ["danted"]
source = """
preserved_fields = {}; preserved_keys = ["host", "timestamp"]

for_each(preserved_keys) -> |_, k| {
  v = get!(., [k])
  if v != null {
    preserved_fields = set!(preserved_fields, [k], v)
  }
}

.message = string!(.message)
addrs = split(.message, " [: ")[-1]
addrs = split(string!(addrs), " ")

client_addr_with_port = split(addrs[0], ".") ?? ["N/A", "N/A"]
server_addr_with_port = split(addrs[1], ".") ?? ["N/A", "N/A"]

. = merge({
  "client_addr": client_addr_with_port[0],
  "client_port": client_addr_with_port[1],
  "server_addr": server_addr_with_port[0],
  "server_port": server_addr_with_port[1],
}, preserved_fields)
"""

# danted (socks proxy) (metrics)

[transforms.danted_metrics]
type = "log_to_metric"
inputs = ["danted_json"]

[[transforms.danted_metrics.metrics]]
type = "counter"
field = "timestamp"
name = "requests_total"

[transforms.danted_metrics.metrics.tags]
hostname = "{{ host }}"
client_addr = "{{ client_addr }}"
server_addr = "{{ server_addr }}"

# danted (socks proxy) (prometheus)

[sinks.prometheus_exporter_danted]
type = "prometheus_exporter"
inputs = ["danted_metrics"]
address = "${DANTED_PROMETHUS_ADDR:?DANTED_PROMETHUS_ADDR must be provided}"
default_namespace = "danted"
suppress_timestamp = true

# this metric sees very low activity,
# therefore we retain it for a longer time (2 hrs)
flush_period_secs = 7200

# certificate-issuer

[transforms.certificate_issuer]
type = "filter"
inputs = ["journal"]
condition = "._SYSTEMD_UNIT == \"certificate-issuer.service\""

[transforms.certificate_issuer_normalized]
type = "remap"
inputs = ["certificate_issuer"]
source = """
. = parse_json!(.message)

.service = "certificate-issuer"
.timestamp = parse_timestamp!(.timestamp, "%+")
"""

# certificate-syncer

[transforms.certificate_syncer]
type = "filter"
inputs = ["journal"]
condition = "._SYSTEMD_UNIT == \"certificate-syncer.service\""

[transforms.certificate_syncer_normalized]
type = "remap"
inputs = ["certificate_syncer"]
source = """
. = parse_json!(.message)

.service = "certificate-syncer"
.timestamp = parse_timestamp!(.timestamp, "%+")
"""

# ic-boundary w/o http/check logs

[transforms.ic_boundary]
type = "filter"
inputs = ["journal"]
condition = "._SYSTEMD_UNIT == \"ic-boundary.service\""

[transforms.ic_boundary_normalized]
type = "remap"
inputs = ["ic_boundary"]
source = """
. = parse_json!(.message)

.service = "ic-boundary"
.timestamp = parse_timestamp!(.timestamp, "%+")
"""

[transforms.ic_boundary_filtered]
type = "filter"
inputs = ["ic_boundary_normalized"]
condition = """
    !includes([
        \"check\",
        \"http_request_in\",
    ], .action)
"""

[sinks.console]
type = "console"
encoding.codec = "json"
inputs = [
  "certificate_issuer_normalized",
  "certificate_syncer_normalized",
  "ic_boundary_filtered",
]
