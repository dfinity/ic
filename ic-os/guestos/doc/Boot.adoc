= IC guest OS boot sequence

The system boot starts with the initramfs which performs some preparatory
operations already. It takes a first look at the +/config+ filesystem to
load a machine-id file if it exists. If it is found, then it is bind-mounted
on top of the root filesystem to provide the machine-id in subsequent boot
(and which would otherwise be randomly generated by systemd once it takes
over). This ensures stability of machine-id after first boot.

After switch to the proper root filesystem, the following essential IC-specific
service are started in the IC-OS boot sequence:

- Mount +/boot+ filesystems (including +/boot/config+)

- Save machine-id

- Set up ssh host keys

- Set up node exporter keys

- Set up key for block device encryption

- Set up and mount +/var+ filesystem

- Set up of logical volume manager

- One-time set up of filesystems in LVs

- Mount of file systems in LVs

- IC node config injection

- Set up ssh account keys

- Generate network configuration

- Set up hostname

- IPv6 address monitor / retry

- Upgrade data store

- nftables reload on change

- Start journalbeat

- Start node exporter

== Mount +/boot+ filesystems

The partition used to be mounted as +/boot+ depends on the partition
set that we are booting from (see link:DiskLayout{outfilesuffix}[disk layout]
for description of partition layout). The mount description for +/boot+ is therefore
not held in +/etc/fstab+ but is generated by the shell script
+/etc/systemd/system-generators/mount-generator+.

Afterwards, the first three partitions are mounted as +/boot/efi+, +/boot/grub+
and +/boot/config+, respectively. The +config+ partition is
used as (small) store for data that is preserved across upgrades
and is available at early boot time already (see link:ConfigStore{outfilesuffix}[config store]).

== Save machine-id

If a machine-id was randomly generated (as happens on first boot), it is saved
away to the +/boot/config+ partition such that it can be restored during
next boot and ensure it is stable across both reboots and upgrades. This
must be restored on next boot from initrd as the root filesystem is read-only
and systemd has no other way of stashing it away otherwise.

== Set up key for block device encryption

Service: +setup-encryption.service+, script: +/opt/ic/bin/setup-encryption.sh+,
depends on +/boot/config+ mount. cryptsetup for +/dev/vda10+ depends on this.

The initially deployed image has only 9 partitions set up. The 10th partition
of the running system is to take up the entire "remainder" of the disk and
be used as encrypted payload storage.

At first boot after deployment, the missing 10th partition will be created,
an encryption key will be generated, and the partition will be set up as
encrypted storage.

== Set up and mount +/var+ filesystem

Partition numbers 6 or 9 (for system A and system B, respectively) are used
for the +/var+ filesystem hierarchy. It is set up as an encrypted filesystem
as well, but its lifetime is limited to the system that it is associated with:
If system A is upgraded to system B, then the +/var+ partition associated of
system B is set up from scratch on first boot of system B. The (now unused)
+/var+ partition of system A will be scrapped and overwritten on next upgrade
written into system A again.

The partition is set up as an encrypted partition as well (since IC intermediary
data might leak to it). This is facilitated in the following way:

* The script +/opt/ic/bin/setup-var-encryption.sh+ will check if the partition
  is set up correctly already. If it is, then it is simply used as-is.
  Otherwise, it is reformatted as an encrypted partition, and a filesystem
  is put in. The filesystem is initialized from the filesystem state of
  the /var subtree that is part of the root filesystem. (So this effectively
  serves as a template defining initial structure of /var).

* The unit file triggering this script is dynamically generated through
  +/etc/systemd/system-generators/mount-generator+: The generater will
  check which partition is the correct one to use and synthesize a proper
  unit file.

When an upgrade is installed into either system slot A or B, it is ensured
that the corresponding +/var+ partition is wiped such that the newly booted
system will set up its own +/var+ filesystem correctly again.

== Set up ssh host keys

Service: +setup-ssh-keys.service+, script: +/opt/ic/bin/setup-ssh-keys.sh+,
depends on +/boot/config+ mount.

This checks if ssh host keys for the system exist in the +config+ partition
(creating them if necessary -- only on first boot after deployment). The
keys are then copied to tmpfs and bind-mounted to +/etc/ssh+. Keeping
host keys in the +config+ partition ensures that they are stable across
system upgrades.

== Set up node exporter keys

Service: +setup-node_exporter-keys.service+, script +/opt/ic/bin/setup-node_exporter-keys.sh+,
depends on +/boot/config+ mount.

This generates the TLS key pair for the +node_exporter+ service on first boot.
The keys are then bind-mounted into a suitable location within +/etc+.

Similar to the ssh keys, the keys are held in the +config+ partition such that
they are persisted across upgrades and available in early boot.

== Set up of logical volume manager

Service: +setup-lvs.service+, script: +/opt/ic/bin/setup-lvs.sh+, depends
on cryptsetup for +/dev/vda10+.

This ensure that the volume group +store+ exists (creating it if it does
not -- this is a one-time action on first boot after provisioning)
and is activated. It then ensure that logical volumes +store/shared-crypto+,
+store/shared-backup+ and +/store/shared-data+ exist (again creating them
if needed, one-time action after boot). These will be used
to hold data that is persisted across backups and held encrypted.

== One-time setup of filesystems in LVs

Services: +setup-shared-data.service+, +setup-shared-crypto.service+, +setup-shared-backup.service+,
scripts +/opt/ic/bin/setup-shared-data.sh+, +/opt/ic/bin/setup-shared-crypto.sh+, +/opt/bin/setup-shared-backup.sh+.
Depend on set up of logical volume manager. Mounts of filesystems in LVs
depend on this.

This checks whether correct filesystems are already set up in the
logical volumes and creates them if not (one-time set up after
provisioning).

== Mount of file systems in LVs

The filesystems mounts are defined in +/etc/fstab+, it is ensured via
dependencies that set up of LVs completes before +fsck+ and +mount+
of these.

== IC node config injection

Service: +bootstrap-ic-node.service+, script +/opt/bin/ic/boostrap-ic-node.sh+,
depends on mount of all filesystems.

This is only executed once on first boot after provisioning. It looks for a "virtual
USB stick" attached to the VM that contains a tar file with initial configuration
for parts of the system (see link:ConfigStore{outfilesuffix}[config store] for a description). Required
files in the +config+ partition as well as payload store are created.

== Deploy updated ssh account keys

Service: +deploy-updated-ssh-account-keys.service+, +deploy-updated-ssh-account-keys.sh+.
Depends on +bootstrap-ic-node.service+, runs before +setup-ssh-account-keys.service+.

Changes the keys held in the +config+ partition for the +backup+ and +readonly+ user. This
is a work-around due to not having a key management solution that updated keys are
deployed via system upgrades.

== Set up ssh account keys

Service: +setup-ssh-account-keys.services+, script +/opt/ic/bin/setup-ssh-account-keys.sh+.
Depends on +bootstrap-ic-node.service+.

The +authorized_keys+ files for the role accounts are taken from the
config partition and bind-mounted into the correct locations in
the account user home directories.

== Generate network configuration

Service: +generate-network-config.service+, script +/opt/ic/bin/generate-network-config.sh+.
Dependes on +bootstrap-ic-node.service+, runs before +systemd-networkd.service+.

This parses the network configuration given in the +config+ partition and
generates network configuration directives for +systemd-networkd+ to apply
later.

== Set up hostname

Service: +generate-network-config.service+, script +/opt/ic/bin/generate-network-config.sh+.
Dependes on +bootstrap-ic-node.service+, runs before +systemd-networkd.service+.

Sets hostname as defined in the +config+ partition.

== Upgrade data store

Service: +upgrade-shared-data-store.service+, script +/opt/ic/bin/upgrade-shared-data-store.sh+.
Depends on mount of requisite filesystem.

This script is intended as a hook to perform any required conversion of the
contents of +/var/lib/ic/data+. Such may be necessary as a one-time change
after upgrade from one system image to another.

== nftables reload on change

Service: +reload_nftables.service+ depending +reload_nftables.path+

This lets systemd monitor the contents of the +nftables.conf+ ruleset file
(dynamically generated by IC stack depending on registry) and issues a reload
command to the nftables subsystem in order to activate the ruleset.

== IPv6 address monitor / retry

Service: +retry-ipv6-config.service+, script +/opt/ic/bin/retry-ipv6-config.sh+.

Periodically checks whether an IPv6 address has been assigned to the primary
interface and issues +networkctl reconfigure+ as needed. The reason is that
+systemd-networkd+ gives up on trying SLAAC autoconfiguration after a while,
so systems will fail to receive network configuration under certain conditions
if the router in their network is down at boot.

== Start journalbeat

Service: +journalbeat.service+. Pre-exec script +/opt/ic/bin/generate-journalbeat-config.sh+.
Depends on +bootstrap-ic-node.service+.

This launches journalbeat (if requested so by injected configuration). The configuration
for the binary itself is generated dynamically from the configuration pieces held
in the +config+ partition.

== Start node_exporter

Service: +node_exporter.service+. Depends on +setup-node_exporter-keys.service+.

Starts the +node_exporter+ service to make machine metrics accessible externally.

== Start IC replica

Service: +ic-replica.service+, pre-exec scripts +/opt/ic/bin/generate-replica-config.sh+ and
+/opt/ic/bin/setup-permissions.sh+. Depends on all file system mounts as well
as having an IPv6 address obtained on primary network interface.

Starts the nodemaneger which in turn monitors and starts the IC replica service.
The first pre-exec script creates the configuration of the replica (which among
other things contains the IPv6 address). The second pre-exec script fixes up
permissions for all files and directories used by replica (this should "probably"
move to a different location, such as "upgrade data store").
